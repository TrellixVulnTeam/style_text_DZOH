{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1.,  2.,  3.]), tensor([ 4.,  5.]), tensor([ 6.])]\n",
      "tensor([[ 1.,  4.,  6.],\n",
      "        [ 2.,  5.,  0.],\n",
      "        [ 3.,  0.,  0.]])\n",
      "PackedSequence(data=tensor([ 1.,  4.,  6.,  2.,  5.,  3.]), batch_sizes=tensor([ 3,  2,  1]))\n",
      "tensor([[ 1.,  4.,  6.],\n",
      "        [ 2.,  5.,  0.],\n",
      "        [ 3.,  0.,  0.]])\n",
      "PackedSequence(data=tensor([ 1.,  4.,  6.,  2.,  5.,  3.]), batch_sizes=tensor([ 3,  2,  1]))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils\n",
    "# %% packed sequences\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5])\n",
    "c = torch.Tensor([6])\n",
    "lengths = list(map(len, [a, b, c]))\n",
    "padded= torch.nn.utils.rnn.pad_sequence([a, b, c], batch_first=False)\n",
    "packed_padded =torch.nn.utils.rnn.pack_padded_sequence(padded, lengths) # non ho messo batch first da qui in poi\n",
    "repadded = torch.nn.utils.rnn.pad_packed_sequence(packed_padded)[0]\n",
    "repacked = torch.nn.utils.rnn.pack_padded_sequence(repadded, lengths)\n",
    "print([a, b, c])\n",
    "print(padded)\n",
    "print(packed_padded)\n",
    "print(repadded)\n",
    "print(repacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parameters import Params\n",
    "from src.vocabulary import Vocabulary\n",
    "from src.style_transfer import StyleTransfer\n",
    "from src.greedy_decoding import Decoder\n",
    "from src.generate_batches import preprocessSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()\n",
    "vocab = Vocabulary()\n",
    "vocab.loadVocabulary(\"data/yelp/vocabulary.pickle\")\n",
    "vocab.initializeEmbeddings(params.embedding_size)\n",
    "model = StyleTransfer(params, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = torch.load(\"data/models/yelp/model-2018-06-27-epoch_18-loss_45.287033\")\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data/yelp/dev/negative.txt', 'r') as fp:\n",
    "    testSents = fp.readlines()[:32]\n",
    "    \n",
    "labels = np.array([0] * len(testSents))\n",
    "testSents = sorted(testSents, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(model, 20, 12, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths:  32\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs, generator_inputs, targets, lenghts = \\\n",
    "            model._sentencesToInputs(testSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it was obvious it was the same damn one he brought the first time <unk> <eos>\n",
      "it 's not really french food and the decor is n't really french either <unk> <eos>\n",
      "waitress ( jen ) was nice but we waited very long for the food <unk> <eos>\n",
      "if we were to get this server again , i 'd ask to move <unk> <eos>\n",
      "we waited over half an hour to get menus and then only got _num_ <unk> <eos>\n",
      "easter day nothing open , heard about this place figured it would ok <unk> <eos> <pad>\n",
      "the host that walked us to the table and left without a word <unk> <eos> <pad>\n",
      "the last couple years this place has been going down hill <unk> <eos> <pad> <pad> <pad>\n",
      "i ordered a chicken sandwich with onion rings and a soda <unk> <eos> <pad> <pad> <pad>\n",
      "short term memory apparently since they were still on main entree <unk> <eos> <pad> <pad> <pad>\n",
      "about _num_ minutes after sitting down , our orders were taken <unk> <eos> <pad> <pad> <pad>\n",
      "last night however it was way to thick and tasteless <unk> <eos> <pad> <pad> <pad> <pad>\n",
      "i love the food ... however service here is horrible <unk> <eos> <pad> <pad> <pad> <pad>\n",
      "i tried to eat it but it was disgusting <unk> <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "i pushed it aside and did n't eat anymore <unk> <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "drive out , pay , grab food and leave <unk> <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "no everytime my food still needs to get ready <unk> <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "ok never going back to this place again <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "do yourself a favor and just stay away <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the total for this lunch was $ _num_ <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "really stressed to leave them off the plate <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "steak cooked wrong , under temp <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "no sign of the manager <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it smelled like rotten urine <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "i will never be back <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "$ _num_ for a soda <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it just gets worse <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the food tasted awful <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "i am not exaggerating <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "this smelled bad <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it tasted horrible <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "not so <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "padded_targets = torch.nn.utils.rnn.pad_packed_sequence(targets, batch_first=True)[0]\n",
    "for i in range(32):\n",
    "    sent = []\n",
    "    for j in range(16):\n",
    "        sent.append(model.vocabulary.id2word[padded_targets[i, j]])\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval_size = len(testSents)\n",
    "model._computeHiddens(\n",
    "                encoder_inputs, generator_inputs, labels, lenghts, True)\n",
    "generatorOutputs, h_teacher = model._generateTokens(\n",
    "            generator_inputs, model.originalHiddens, lenghts, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 9603])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generatorOutputs.view(-1, model.vocabulary.vocabSize).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.rnn.pad_packed_sequence(targets, batch_first=True)[0].contiguous().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2853, device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packedGenOutput = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "    generatorOutputs, lenghts, batch_first=True)[0] # PROVA CON batch_first = True, sballa l'ordine!\n",
    "\n",
    "model.rec_loss_criterion(\n",
    "    generatorOutputs.view(-1, model.vocabulary.vocabSize),\n",
    "    torch.nn.utils.rnn.pad_packed_sequence(targets, batch_first=True)[0].contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with previous Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rnn import SoftSampleWord\n",
    "\n",
    "def _generateWithPrevOutput(\n",
    "            model, h0, max_len, size, lengths=[], evaluation=False, soft=True):\n",
    "\n",
    "    hidden = h0\n",
    "    hiddens = torch.zeros(size, max_len,\n",
    "                          model.params.autoencoder.hidden_size,\n",
    "                          device=\"cuda\")\n",
    "    if soft:\n",
    "        tokens = torch.zeros(\n",
    "            size, max_len, model.params.embedding_size, device=\"cuda\")\n",
    "    else:\n",
    "        tokens = torch.zeros(size, max_len, device=\"cuda\")\n",
    "        \n",
    "    print(tokens.shape)\n",
    "\n",
    "    goEmbedding = model.vocabulary(['<go>']).squeeze(0)\n",
    "    goEmbedding = goEmbedding.repeat(size, 1)\n",
    "    goEmbedding = goEmbedding.unsqueeze(1)\n",
    "    currTokens = goEmbedding\n",
    "    softSampleFunction = SoftSampleWord(\n",
    "        dropout=model.params.dropout,\n",
    "        embeddings=model.vocabulary.embeddings,\n",
    "        gamma=model.params.gamma_init)\n",
    "    \n",
    "    if soft:\n",
    "\n",
    "        for index in range(max_len):\n",
    "            # generator need input (seq_len, batch_size, input_size)\n",
    "            output, hidden = model.generator(\n",
    "                currTokens, hidden, pad=False)\n",
    "            currTokens, vocabLogits = softSampleFunction(\n",
    "                output=output,\n",
    "                hiddenToVocab=model.hiddenToVocab)\n",
    "            tokens[:, index, :] = currTokens\n",
    "            currTokens = currTokens.unsqueeze(1)\n",
    "            hiddens[:, index, :] = hidden\n",
    "            \n",
    "    else:\n",
    "        for index in range(max_len):\n",
    "            output, hidden = model.generator(currTokens, hidden, pad=False)\n",
    "            vocabLogit = model.hiddenToVocab(hidden)\n",
    "            idxs = vocabLogit[0, : , :].max(1)[1]\n",
    "            outputs[:, i] = idxs\n",
    "            currTokens = model.vocabulary(idxs).unsqueeze(1)\n",
    "\n",
    "    hiddens = torch.cat((h0.transpose(0, 1), hiddens), dim=1)\n",
    "    # tokens = torch.cat((goEmbedding, tokens), dim=1)\n",
    "    return hiddens, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs, generator_inputs, targets, lenghts = \\\n",
    "            model._sentencesToInputs(testSents)\n",
    "\n",
    "model.eval_size = len(testSents)\n",
    "model._computeHiddens(\n",
    "                encoder_inputs, generator_inputs, labels, lenghts, True)\n",
    "\n",
    "generateWithPrevOutputs, h_prof = _generateWithPrevOutput(\n",
    "    model, model.originalHiddens, model.params.max_len, 32, lenghts, True, soft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 32\n",
    "goEmbedding = model.vocabulary(['<go>']).squeeze(0)\n",
    "goEmbedding = goEmbedding.repeat(size, 1)\n",
    "goEmbedding = goEmbedding.unsqueeze(1)\n",
    "currTokens = goEmbedding\n",
    "outputs = torch.zeros(32, 20)\n",
    "hidden = model.originalHiddens\n",
    "for i in range(20):\n",
    "    # generator need input (seq_len, batch_size, input_size)\n",
    "    output, hidden = model.generator(\n",
    "        currTokens, hidden, pad=False)\n",
    "    currTokens, vocabLogits = softSampleFunction(\n",
    "        output=output,\n",
    "        hiddenToVocab=model.hiddenToVocab)\n",
    "    tokens[:, i, :] = currTokens\n",
    "    currTokens = currTokens.unsqueeze(1)\n",
    "    hiddens[:, i, :] = hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for i in range(32):\n",
    "    tokens = outputs[i, :]\n",
    "    words = [model.vocabulary.id2word[int(x)] for x in list(tokens)]\n",
    "    sents.append(\" \".join(words))\n",
    "    \n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for index in range(32):\n",
    "    tokensLogits = generateWithPrevOutputs[index, :, :]\n",
    "    sent = []\n",
    "    len = 20\n",
    "    for j in range(len):\n",
    "        logit = tokensLogits[j, :]\n",
    "        sent.append(model.vocabulary.id2word[logit.argmax()])\n",
    "    sents.append(\" \".join(sent))\n",
    "sents        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(packedGenOutput.shape)\n",
    "print(targets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for index in range(packedGenOutput.shape[0]):\n",
    "    tokensLogits = packedGenOutput[index]\n",
    "    sent = []\n",
    "    sent.append(model.vocabulary.id2word[tokensLogits.argmax()])\n",
    "    sents.append(\" \".join(sent))\n",
    "sents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for index in range(targets.shape[0]):\n",
    "    word = targets[index]\n",
    "    sent = []\n",
    "    sent.append(model.vocabulary.id2word[word])\n",
    "    sents.append(\" \".join(sent))\n",
    "sents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, tsf = decoder.rewriteBatch(testSents[:2], labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(torch.FloatTensor([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'the restaurant was nice but food tasted ugly'.split()\n",
    "h = torch.zeros(700).to('cuda')\n",
    "h = h.unsqueeze(0).unsqueeze(0)\n",
    "for token in sent:\n",
    "    emb = vocab([token])\n",
    "    emb = emb.unsqueeze(1)\n",
    "    out, h = model.generator(emb, h, pad=False)\n",
    "    voc = model.hiddenToVocab(out)\n",
    "    _, id = voc.max(2)\n",
    "    h = h\n",
    "    print(vocab.id2word[int(id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hiddenToVocab(out).max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
